# Kernerl Regression Tree
## Junting Ren
This repository contains R code for implementing kernel regression tree and its comparison between boosting, random forest and KNN predictions in both simulated data and real data.

Regression trees provide quite simple and interpretable regression models with reasonable accuracy. However, these methods are known for their instability. Weiss & Indurkhya (1995) used a nearest neighbor approximation in their regression rules inductive system. Quinlan (1992) and Karalic (1992) have used linear models in regression tree leaves. Torgo (1999) used kernel regression models on data in the corresponding leaves to predict outcome (Local regression tree). Torgos method showed superiority compared to the traditional method that use mean in the leaves to predict outcome, but it also requires signiﬁcantly more local memory and computational time. However, through some searching, I did not ﬁnd articles that compare it with random forest and regression tree bagging. Furthermore, existing R packages do not provide kernel regression predictions for regression trees.

The intuition for this method is that the traditional tree algorithm only used the mean of the outcome of the corresponding leaf’s data to do prediction. The information of the variables’ are not utilized in the ﬁnal prediction. Moreover, some variables are not used in the spliting, but they can still provide some information for prediction. Therefore, we would like to use all the predictors available in the ﬁnal prediction in the leaf to get a more robust prediction. By preserving the predictors’ data in the leaf, given a new data point, we can calculate the distances(based on diﬀerent predictors) to the training data points, and using this distance to weight the outcome in the ﬁnal prediction.
